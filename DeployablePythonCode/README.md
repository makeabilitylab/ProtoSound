  
# ProtoSound #  
  
![Status](https://img.shields.io/badge/Version-Experimental-brightgreen.svg)  
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)  
  
Introduction  
------------  

This is a PyTorch-based implementation of ProtoSound with our pre-trained MobileNetV2 model. This code can be deployed on any device with a Python interpreter (e.g., a raspeberry pi, a desktop, or a python-based IoT device). 

The code supports any number of classes (ways) or samples per class (shots).
  
[[Website](https://makeabilitylab.cs.washington.edu/project/soundwatch/)]  
[[Paper PDF](https://homes.cs.washington.edu/~djain/img/portfolio/Jain_ProtoSound_CHI2022.pdf)]  
[[Video](https://homes.cs.washington.edu/~djain/img/portfolio/protosound-video.mp4)]  
  
Requirements 
--------------  
- Python >= 3.6
- PyTorch
- Librosa
- Pandas
- OS, tqdm